# ğŸ§ ğŸ¨ Pendekatan 1: CLIP sebagai Semantic Loss Function

## ğŸ“‹ Overview

Pendekatan ini mengintegrasikan CLIP sebagai **fungsi kerugian semantik** dalam arsitektur VAE-GAN untuk rekonstruksi EEG-to-Image. Alih-alih hanya mengandalkan pixel-level reconstruction loss, model juga belajar menghasilkan gambar yang **secara semantik benar** menurut CLIP.

### ğŸ—ï¸ Arsitektur

```
EEG Signal (264-dim)
         â†“
    EEG Encoder (VAE)
    â”œâ”€â”€ Î¼, Ïƒ (latent parameters)
    â””â”€â”€ z ~ N(Î¼, ÏƒÂ²) (reparameterization)
         â†“
    Image Generator/Decoder
         â†“
    Generated Image (224Ã—224Ã—3)
         â†“
    [CLIP Frozen Encoder] â† Target Image
         â†“                      â†“
    Generated Embedding    Target Embedding
         â†“                      â†“
         â””â”€â”€â”€ Semantic Loss â”€â”€â”€â”€â”˜
```

### ğŸ¯ Loss Function

**Total Loss = Î»_adv Ã— L_GAN + Î»_rec Ã— L_REC + Î»_kl Ã— L_KL + Î»_clip Ã— L_CLIP**

Dimana:
- **L_GAN**: Adversarial loss (Generator vs Discriminator)
- **L_REC**: Pixel-level reconstruction loss (L1)
- **L_KL**: VAE KL divergence loss
- **L_CLIP**: **Semantic loss** = 1 - cosine_similarity(generated_embedding, target_embedding)

### âœ¨ Keunggulan Pendekatan 1

1. **Semantic Correctness**: Model belajar menghasilkan gambar yang **konseptually correct**
2. **Mengatasi Blur**: Semantic loss tidak terlalu "menghukum" detail kecil yang salah
3. **Stable Training**: Arsitektur VAE-GAN yang proven dengan tambahan semantic guidance
4. **TRUE Labels**: Menggunakan subject-stimulus correspondence yang benar

## ğŸš€ Quick Start

### Step 1: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 2: Run Training
```bash
python run_approach1_pipeline.py
```

## ğŸ“Š Expected Results

### Performance Metrics:
- **Semantic Accuracy**: 60-80% (berdasarkan CLIP similarity)
- **Top-3 Stimulus Match**: 50-70%
- **Visual Quality**: High semantic correctness, good detail preservation

### Generated Output:
- **Format**: [EEG Signal] [TRUE Ground Truth] [Generated Image] [Top-3 Predictions]
- **Borders**: Green (TRUE), Gold/Silver/Bronze (predictions), âœ… (correct)

## ğŸ”§ Configuration

Edit `run_approach1_pipeline.py`:

```python
config = {
    # Loss weights
    'lambda_adv': 1.0,      # GAN adversarial loss
    'lambda_rec': 10.0,     # Reconstruction loss
    'lambda_kl': 1.0,       # VAE KL loss
    'lambda_clip': 5.0,     # CLIP semantic loss (KEY!)
    
    # Training
    'batch_size': 16,       # Smaller for memory
    'num_epochs': 20,       # More epochs for convergence
    'learning_rate': 2e-4,  # Standard GAN learning rate
}
```

## ğŸ“ File Structure

```
approach1_clip_semantic_loss/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ run_approach1_pipeline.py          # ğŸ¯ MAIN SCRIPT
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ datasets/
â”‚       â”œâ”€â”€ EP1.01.txt                 # MindbigData
â”‚       â””â”€â”€ *.jpg                      # Stimulus images
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ semantic_loss_model.py         # VAE-GAN + CLIP model
â”‚   â”œâ”€â”€ data_processing.py             # Dataset processing
â”‚   â”œâ”€â”€ training.py                    # Training pipeline
â”‚   â”œâ”€â”€ evaluation.py                  # Evaluation & visualization
â”‚   â””â”€â”€ utils.py                       # Utilities
â””â”€â”€ results/                           # Output folder
    â”œâ”€â”€ models/                        # Trained models
    â”œâ”€â”€ visualizations/                # Generated images
    â””â”€â”€ logs/                          # Training logs
```

## ğŸ¨ Key Innovation

**Semantic Loss Function**:
```python
def semantic_loss_function(generated_images, target_images, clip_model):
    # Get CLIP embeddings
    gen_embeddings = clip_model.encode_image(generated_images)
    target_embeddings = clip_model.encode_image(target_images)
    
    # Cosine similarity loss
    cosine_sim = F.cosine_similarity(gen_embeddings, target_embeddings)
    semantic_loss = 1 - cosine_sim.mean()
    
    return semantic_loss
```

Ini memaksa generator menghasilkan gambar yang **semantically similar** dengan target, bukan hanya pixel-perfect.

## ğŸ”¬ Technical Details

### Model Components:
1. **EEG Encoder**: Transforms EEG â†’ latent space (VAE)
2. **Image Generator**: Transforms latent â†’ RGB image (ConvTranspose2d)
3. **Discriminator**: Real/fake image classification
4. **CLIP Encoder**: Frozen pre-trained, untuk semantic loss

### Training Process:
1. **Phase 1**: Train Discriminator (real vs fake)
2. **Phase 2**: Train Generator with combined loss:
   - Fool discriminator (adversarial)
   - Reconstruct pixels (L1)
   - Maintain latent distribution (KL)
   - **Match semantic content (CLIP)**

## ğŸ¯ Expected Advantages

1. **Better Semantic Quality**: Gambar yang dihasilkan lebih "meaningful"
2. **Reduced Blur**: Semantic loss membantu detail yang sharp
3. **Robust to Pixel Variations**: Focus pada konsep, bukan pixel exact
4. **Improved Generalization**: CLIP knowledge membantu unseen variations

## ğŸš€ Ready to Run!

Folder ini siap dijalankan dengan:
```bash
python run_approach1_pipeline.py
```

Akan menghasilkan rekonstruksi EEG-to-Image dengan **semantic guidance dari CLIP**! ğŸ§ âœ¨ğŸ¨
